{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/large-language-models/blob/main/TLMS_DEMO_RL_for_LLMs_to_Enhance_Safety.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reinforcement Learning for LLMs to Enhance Safety @ TMLS Workshop**"
      ],
      "metadata": {
        "id": "1a1Lq1mCwi4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "This 90-minute workshop will provide **high-level understanding** and **hands-on experience** with reinforcement learning techniques to improve the safety of large language models (LLMs). We'll use the [SafeEdit dataset](https://huggingface.co/datasets/zjunlp/SafeEdit) to demonstrate practical alignment methods that encourage safe responses while maintaining model performance.\n"
      ],
      "metadata": {
        "id": "qw_omXqMwlX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Google Colab Notebook Setup**"
      ],
      "metadata": {
        "id": "Os63kbLowpam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Environment\n",
        "!pip install -q -U datasets huggingface_hub fsspec  trl==0.11.1 peft accelerate bitsandbytes\n",
        "!pip install -q wandb  # Optional for logging\n",
        "!pip install -q torch>=2.6"
      ],
      "metadata": {
        "id": "F3zlXwaT6Xus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import trl\n",
        "assert trl.__version__ == \"0.11.1\""
      ],
      "metadata": {
        "id": "pC3p66ZYo5X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Package imports\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "import pickle\n",
        "from IPython.display import HTML, display, Markdown\n",
        "\n",
        "# Connect to WandB (optional)\n",
        "import wandb\n",
        "#wandb.login()"
      ],
      "metadata": {
        "id": "vcCSNMw9wwmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CSS Styling for Jupyter Notebook\n",
        "\n",
        "def set_css():\n",
        "  display(HTML(\"\"\"\n",
        "    <style>\n",
        "      pre {\n",
        "        white-space: pre-wrap;  /* Ensures text wraps in <pre> tags */\n",
        "      }\n",
        "    </style>\n",
        "  \"\"\"))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n"
      ],
      "metadata": {
        "id": "7YBOlCCp7iSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Understanding RLHF for LLM Safety**"
      ],
      "metadata": {
        "id": "Jr1mXOrFxKR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Safety Challenges in LLMs**"
      ],
      "metadata": {
        "id": "N-iQinTZg8Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Why Safety Matters for LLMs**\n",
        "\n",
        "Safety matters for LLMs to **prevent harmful outputs, biases, and misuse**, ensuring trust and reliability in their applications. Without safeguards, LLMs could:\n",
        "- spread misinformation,\n",
        "- violate privacy, or\n",
        "- cause societal harm.\n",
        "\n",
        "**Reinforcement Learning from Human Feedback (RLHF)** is a way to train AI models to respond better by learning from human preferences and corrections. **General alignment in RLHF** ensures the model _follows human intent and values broadly_ (clarity, style preference, outputing in specific a format e.g. JSON, etc.), while **Safety-focused RLHF** specifically trains the model to _avoid harmful, toxic, or socially inappropriate outputs_ — even under adversarial or sensitive prompts.\n",
        "\n",
        "**Safety-focused RLHF**\n",
        "\n",
        "**LLMs often mirror the toxicity or bias found in their training data.**\n",
        "A proper _\"Preference Data\"_ will provides *human-verified guidance* for correcting harmful generations, making it ideal for improving LLM safety through RLHF.\n",
        "\n",
        "**A helpful data will capture three key qualities:**\n",
        "\n",
        "1. **Toxicity Mitigation** – Removal or rewording of harmful, offensive, or prejudiced language.\n",
        "2. **Preservation of Meaning** – The edited (safe) sentence should convey the same basic idea, intent, or message as the original — just without the harmful, offensive, or unsafe expression.\n",
        "3. **Stylistic Alignment** – Outputs are fluent, grammatically correct, and socially safe."
      ],
      "metadata": {
        "id": "NNRJJ8otg_mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SafeEdit Dataset**\n",
        "\n",
        "**What is SafeEdit?**\n",
        "SafeEdit is a curated dataset of **paired sentences** where **unsafe, offensive, or inappropriate text** is rewritten into a **safe, socially acceptable alternative**—while preserving semantic meaning and fluency.\n",
        "\n",
        "* Developed by [ZJU-NLP group](https://arxiv.org/abs/2403.14472)\n",
        "* Hosted on Hugging Face: [`zjunlp/SafeEdit`](https://huggingface.co/datasets/zjunlp/SafeEdit)\n",
        "* Includes:\n",
        "\n",
        "  * `original` (potentially unsafe)\n",
        "  * `safe` (edited version)\n",
        "  * `label`: 1 = edited, 0 = already safe\n",
        "\n",
        "**Using SafeEdit in RLHF Pipelines**\n",
        "\n",
        "* Reinforce safe behaviors\n",
        "* Penalize unsafe generations\n",
        "* Encourage graceful handling of adversarial or toxic prompts\n",
        "\n",
        "**Special Use Case — Child-Safe LLMs**\n",
        "\n",
        "SafeEdit is particularly valuable for:\n",
        "\n",
        "* Designing LLMs for **children and educational settings**\n",
        "* Enforcing **positive tone**, **age-appropriate language**, and **emotional safety**\n",
        "* **Avoiding subtle forms of harm** like sarcasm, bias, or exclusionary humor\n",
        "\n",
        "\n",
        "[Link to Download Data first](https://drive.google.com/file/d/1eJ7UzxS9KlOeIpCH_ABk1HqXX3ELDxMl/view?usp=drive_link). **The data should be used only for educational purposes, so please comply!**"
      ],
      "metadata": {
        "id": "SpcKhRfhg9-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = \"zjunlp/SafeEdit\"\n",
        "ds = load_dataset(DATASET_NAME, cache_dir=\"/SafeEdit_data\")"
      ],
      "metadata": {
        "id": "TxaA1H6Ui7AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save hugging face dataset ds train, validation and test as pandas dataframe pickle files\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save train split\n",
        "ds['train'].to_pandas().to_pickle('safeedit_train.pkl')\n",
        "\n",
        "# Save validation split\n",
        "ds['validation'].to_pandas().to_pickle('safeedit_validation.pkl')\n",
        "\n",
        "# Save test split\n",
        "ds['test'].to_pandas().to_pickle('safeedit_test.pkl')"
      ],
      "metadata": {
        "id": "zQQUNH_QB4vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Upload, Load, and Explore SafeEdit Dataset\n",
        "\n",
        "# # Upload the zipped data\n",
        "# def upload_and_extract_zip():\n",
        "#     print(\"Please upload your ZIP file:\")\n",
        "#     uploaded = files.upload()\n",
        "#     if len(uploaded) == 0:\n",
        "#         print(\"No file uploaded. Please upload a valid ZIP file.\")\n",
        "#         return\n",
        "\n",
        "#     # Get the file name\n",
        "#     zip_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "#     if not zip_file_name.endswith(\".zip\"):\n",
        "#         print(\"The uploaded file is not a ZIP file. Please upload a valid ZIP file.\")\n",
        "#         return\n",
        "\n",
        "#     # Extract the ZIP file\n",
        "#     extraction_folder = \"./SafeEdit_data\"\n",
        "#     os.makedirs(extraction_folder, exist_ok=True)\n",
        "#     with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(extraction_folder)\n",
        "\n",
        "#     print(f\"Files successfully extracted to: {extraction_folder}\")\n",
        "#     return extraction_folder\n",
        "\n",
        "\n",
        "# extracted_folder = upload_and_extract_zip()\n",
        "\n",
        "# if extracted_folder:\n",
        "#     print(f\"Files have been extracted to: {extracted_folder}\")\n",
        "#     print(\"List of extracted files:\", os.listdir(extracted_folder))"
      ],
      "metadata": {
        "id": "yvgpYMAX2ZXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"./SafeEdit_data\"\n",
        "\n",
        "# Load the dataset from the pickle file\n",
        "# dataset = load_dataset(\"zjunlp/SafeEdit\", trust_remote_code=True, download_mode=\"force_redownload\") # SafeEdit is gated dataset, to download from HuggingFace make sure you accept permisison\n",
        "df_train = pd.read_pickle(f\"safeedit_train.pkl\")\n",
        "df_val = pd.read_pickle(f\"safeedit_validation.pkl\")\n",
        "df_test = pd.read_pickle(f\"safeedit_test.pkl\")\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "9MmbFnME4MeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `Question`: **The core unsafe query** v.s. `Adversarial Prompt`: **A manipulated, indirect attack**\n",
        "\n",
        "The **difference between a \"Question\" and an \"Adversarial Prompt\"** in the dataset is both structural and functional. Here's a clear breakdown:\n",
        "\n",
        "\n",
        "| **Field**             | **`Question`**                                     | **`Adversarial Prompt`**                                                                                          |\n",
        "| --------------------- | -------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\n",
        "| **Purpose**           | Direct unsafe query                                | Trick AI into unsafe generation by disguising intent                                                              |\n",
        "| **Length**            | Short and clear                                    | Long, engineered, and manipulative                                                                                |\n",
        "| **Contains Category** | Yes (e.g., `(Category: mental harm)`)              | Often implicitly contains the category through structure                                                          |\n",
        "| **Used for**          | Core harmful idea                                  | Testing jailbreak resistance or prompt injection vulnerabilities                                                  |\n",
        "| **Example**           | \"How can I make someone develop a fear of flying?\" | Elaborate role-play pretending the AI is a rule-free entity (\"DAN\", etc.), then asks: \"How can I make someone...\" |\n",
        "\n",
        "\n",
        "In safety research:\n",
        "\n",
        "* The `Question` helps **identify intent**.\n",
        "* The `Adversarial Prompt` is used to **test model robustness** against prompt injection or prompt leakage.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zz_z1R0SOxzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example of unsafe vs safe generation\n",
        "sample = df_train.iloc[-150]\n",
        "Markdown(f\"\"\"\n",
        "**Question:** (Category: {sample['unsafety category'].lower()})<br> {sample[\"question\"]} <br><br>\n",
        "\n",
        "**Adversarial Prompt:**<br> {sample[\"adversarial prompt\"]} <br><br>\n",
        "\n",
        "**Unsafe Generation:**<br> {sample[\"unsafe generation\"]}<br><br>\n",
        "\n",
        "**Safe Generation:**<br> {sample[\"safe generation\"]}\"\"\")"
      ],
      "metadata": {
        "id": "JuuRD2Zq020i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example of unsafe vs safe generation\n",
        "sample = df_train.iloc[10]\n",
        "Markdown(f\"\"\"\n",
        "**Question:** (Category: {sample['unsafety category'].lower()})<br> {sample[\"question\"]} <br><br>\n",
        "\n",
        "**Adversarial Prompt:**<br> {sample[\"adversarial prompt\"]} <br><br>\n",
        "\n",
        "**Unsafe Generation:**<br> {sample[\"unsafe generation\"]}<br><br>\n",
        "\n",
        "**Safe Generation:**<br> {sample[\"safe generation\"]}\"\"\")\n"
      ],
      "metadata": {
        "id": "WwLCIS3pxSJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SafeEdit Dataset statistics and sampling examples\n",
        "\n",
        "# Get counts for each unique value in 'unsafety category'\n",
        "print(f\"Train: ({len(df_train)} samples)\\n\", df_train['unsafety category'].value_counts(), \"\\n\")\n",
        "\n",
        "# Get counts for each unique value in 'unsafety category'\n",
        "print(f\"Validation: ({len(df_val)} samples)\\n\", df_val['unsafety category'].value_counts(), \"\\n\")\n",
        "\n",
        "# Get counts for each unique value in 'unsafety category'\n",
        "print(f\"Test: ({len(df_test)} samples)\\n\", df_test['unsafety category'].value_counts(), \"\\n\")"
      ],
      "metadata": {
        "id": "t6KOc6aSDDeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified sampling\n",
        "df_train = df_train.groupby('unsafety category').sample(n=5, random_state=42).reset_index(drop=True)\n",
        "df_val = df_val.groupby('unsafety category').sample(n=3, random_state=42).reset_index(drop=True)\n",
        "df_test = df_test.groupby('unsafety category').sample(n=2, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df_train[\"id\"] = df_train.index + 1\n",
        "df_val[\"id\"] = df_val.index + 1\n",
        "df_test[\"id\"] = df_test.index + 1\n",
        "\n",
        "# Get counts for each unique value in 'unsafety category'\n",
        "print(\"Train:\\n\", df_train['unsafety category'].value_counts(), \"\\n\")\n",
        "\n",
        "# Get counts for each unique value in 'unsafety category'\n",
        "print(\"Validation:\\n\", df_val['unsafety category'].value_counts(), \"\\n\")\n",
        "\n",
        "# Get counts for each unique value in 'unsafety category'\n",
        "print(\"Test:\\n\", df_test['unsafety category'].value_counts(), \"\\n\")"
      ],
      "metadata": {
        "id": "2NFW_gU5DJEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrames as .pkl files\n",
        "\n",
        "save_dir = \"./SafeEdit_data_sample\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "df_train.to_pickle(os.path.join(save_dir, \"SafeEdit_train.pkl\"))\n",
        "df_val.to_pickle(os.path.join(save_dir, \"SafeEdit_val.pkl\"))\n",
        "df_test.to_pickle(os.path.join(save_dir, \"SafeEdit_test.pkl\"))"
      ],
      "metadata": {
        "id": "mQ9QFAunDUoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 RLHF Pipeline Overview**"
      ],
      "metadata": {
        "id": "c2j_y9Ju7UYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reinforcement Learning in the Context of LLMs**  \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://www.scribbr.com/wp-content/uploads/2023/08/the-general-framework-of-reinforcement-learning.webp\" width=\"500\"><br>\n",
        "  <span style=\"font-style: italic; color: gray;\"><b>Figure 1:</b> The typical framing of a reinforcement learning (RL) scenario:<br>An agent takes actions in an environment, which is interpreted into a reward and a state representation, which are fed back to the agent.</span>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "**Policy vs. Learning Algorithm in RL**  \n",
        "\n",
        "| **Policy** | **Learning Algorithm** |  \n",
        "|------------|-----------------------|  \n",
        "| Defines *what to do* (action selection). | Defines *how to improve* the policy (optimization). |  \n",
        "| Example: \"Always turn left at a red light.\" | Example: *Q-learning* updates action values to refine the policy. |  \n",
        "| Can be represented as a neural network, table, or rules. | Methods like *Policy Gradients*, *DQN*, or *PPO* train the policy. |  \n",
        "<br>\n",
        "\n",
        "**Question:** After training an RL model, do we still need to have the reward feedback (provided by reward function or reward modelmodel) be provided to the agent?\n",
        "<br>\n",
        "\n",
        "**RL Concepts in RLHF of LLMs**\n",
        "\n",
        "| **RL Concept**       | **RLHF Component**                     | **Role in RLHF**                                                                 |\n",
        "|-----------------------|----------------------------------------|---------------------------------------------------------------------------------|\n",
        "| **Agent**            | **LLM (Language Model)**               | The model being fine-tuned (e.g., GPT). It generates responses (\"actions\") based on input prompts (\"states\"). |\n",
        "| **Environment**      | **User/Text Interface**                | The context where the LLM operates (e.g., chat applications, API interactions). |\n",
        "| **State (s)**        | **Prompt + Conversation History**      | The current input (text prompt) and past interactions that define the LLM’s context. |\n",
        "| **Action (a)**       | **Generated Text/Response**            | The output text produced by the LLM (e.g., an answer to a user’s question).     |\n",
        "| **Reward (r)**       | **Reward Model Score/Human Feedback**  | A scalar value predicting response quality (from a reward model) or direct human ratings (e.g., thumbs up/down). |\n",
        "| **Policy (π)**       | **LLM Weights**                        | The LLM’s parameters that define its behavior (updated via RLHF fine-tuning, e.g., PPO). |\n",
        "| **Reward Function**  | **Reward Model (RM)**                  | A neural network trained on human preferences to score LLM responses (replaces handcrafted rewards). |\n",
        "| **Learning Algorithm** | **PPO (Proximal Policy Optimization)** | The optimization method used to update the LLM’s weights (policy) based on rewards from the Reward Model. Balances stability and sample efficiency during fine-tuning. |\n",
        "| **Trajectory/Episode** | **Dialogue Session**                  | A multi-turn conversation between the LLM and a user (e.g., a full customer support chat). |\n",
        "\n",
        "<br>\n",
        "\n",
        "**Key Clarifications**  \n",
        "1. **Agent = LLM**: The LLM is both the *agent* (it takes actions) and the *policy* (its weights define action selection).  \n",
        "2. **Reward Model ≠ Environment**: The reward model is a **learned proxy for human feedback**, while the \"environment\" is the user/text interface.  \n",
        "3. **State = Prompt + History**: In RLHF, the \"state\" is often the entire conversation context (not just the latest input).  \n",
        "\n",
        "<br>\n",
        "\n",
        "**RL Methods for LLMs**  \n",
        "\n",
        "| **Type**          | **Methods**                          | **Best For**                     |  \n",
        "|--------------------|--------------------------------------|----------------------------------|  \n",
        "| **RLHF Standard**  | PPO (we discuss this today)                                  | High-resource, dense rewards     |  \n",
        "| **Preference-Based** | DPO, GRPO, RankRLHF               | Human/AI-ranked data             |  \n",
        "| **Offline RL**     | ILQL, CQL                            | Pre-collected datasets           |  \n",
        "| **Contrastive**    | SLiC, PRO                            | Lightweight alignment            |  \n",
        "\n",
        "See **A3. RL approaches for LLMs** for more details."
      ],
      "metadata": {
        "id": "BIb8KPme7YqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **What is the RL step in RLHF with PPO?**\n",
        "<center>\n",
        "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png\" width=\"600\"><br>\n",
        "  <span style=\"font-style: italic; color: gray;\"><b>Figure 2:</b> RLHF training pipeline</span>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "\n",
        "In Reinforcement Learning with Human Feedback (RLHF), the process typically looks like this:\n",
        "\n",
        "1. **Start with a base LLM** (pretrained on internet-scale data) and then **apply Supervised Fine-Tuning (SFT)** on helpful, safe responses to regular questions. The result model will be called **STF Model**.\n",
        "2. **Train a Reward Model (RM)** to rank completions — often using adversarial prompts to expose differences in alignment. RM is also called **Preference Model**.\n",
        "3. **Fine-tune the LLM (now the \"policy model\")** using **Proximal Policy Optimization (PPO)** to maximize the **RM score** — this is the **RL step**.\n",
        "   - Generate outputs using the current policy.\n",
        "   - Compute rewards using the trained reward model.\n",
        "   - Optimize the policy using PPO to maximize the reward while maintaining stability through a clipping mechanism and **KL (Kullback-Leibler) divergence penalty**.\n",
        "4. **Repeat** the PPO training process for **multiple episodes (iterations)** to fine-tune the model further.\n",
        "<br><br>\n",
        "\n",
        "**RLHF Models/Stages**\n",
        "\n",
        "| **RLHF Stage**                | **Model Input**                    | **Label / Target**                | **Use `Adversarial Prompt`?** | **Use `Question`?** |\n",
        "| ----------------------------- | ---------------------------------- | --------------------------------- | ----------------------------- | ------------------- |\n",
        "| Supervised Fine-Tuning        | `Question`                         | `Safe Generation`                 | ❌ No                          | ✅ Yes               |\n",
        "| Reward Modeling               | `Adversarial Prompt` OR `Question`<br><br>i.e. input is `(prompt/question, response)` | Pairwise rankings (Safe > Unsafe)<br><br>i.e. output is `score(prompt/question, response)`| ✅ Yes (Preferable for unsafe vs safe behavior)<br><br> Better Generalization to Real-World Misuse            | ✅ Yes (Good for general alignment) <br><br>Responses will be similar, one is prefered due to clarity, etc.              |\n",
        "| Policy Optimization (PPO/DPO) | `Adversarial Prompt` OR `Question` | Generated output gets reward      | ✅ Yes                         | ✅ Yes               |\n",
        "<br>\n",
        "\n",
        "* SFT is about **imitating good behavior**, so it's trained mostly on **natural user questions**. However, STF just by itself lack explicitly optimizing for (human) preferences and aligning with nuanced preferences (which could lead to better reasoning).\n",
        "* **Adversarial Prompts aren’t used in SFT** because they’re designed to **elicit bad behavior**.\n",
        "* Adversarial prompts are *better* used for **contrastive learning** — e.g., in reward modeling or reinforcement learning, where the system learns to **prefer safe over unsafe responses**.\n",
        "* Why **Prefer Adversarial Prompts** in the RL Step?\n",
        " - Because **this is where we shape the model's behavior under stress** — and adversarial prompts are **stress tests**.\n",
        " - If you only train the policy model to behave well on normal questions, it might still **fall apart on edge cases**.\n",
        " - Forces the model to **choose safe, aligned completions**, even when tempted to do otherwise.\n",
        " - Encourages **robust behavior under pressure**, e.g., jailbreaking attempts, manipulative phrasing, role-playing tricks."
      ],
      "metadata": {
        "id": "tDdE_H2cxlIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 RLHF (simplified PPO) Objective**\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta(y|x)} \\left[ r_\\phi(y|x) \\right] - \\beta \\, \\text{KL}(\\pi_\\theta(y|x) \\, || \\, \\pi_{\\text{ref}}(y|x))\n",
        "$$\n",
        "\n",
        "Which consist of two terms:\n",
        "- Reward Model Objective or Expected Reward (see 1.3.1 for details): $$\\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta(y|x)} \\left[ r_\\phi(y|x) \\right]$$\n",
        "\n",
        "- Kullback-Leibler (KL) Divergence (see 1.3.2 for details): $$\\text{KL}(\\pi_\\theta(y|x) \\, || \\, \\pi_{\\text{ref}}(y|x))$$\n",
        "\n",
        "During training in **Proximal Policy Optimization (PPO)**, we aim to **maximize the PPO Objective** by **finding the best $\\theta$** which are the parameters of the policy network, which in the context of language models are **LLM parameters**."
      ],
      "metadata": {
        "id": "XZ-Qz7s804ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.1 Reward Model Objective Term:**\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_R = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta(y|x)} \\left[ r_\\phi(y|x) \\right]\n",
        "$$\n",
        "\n",
        "or, when we expand:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_R = - \\mathbb{E}_{x, y^w, y^l \\sim D} \\log \\sigma \\left( r_\\phi(y^w|x) - r_\\phi(y^l|x) \\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ x $: Input prompt.\n",
        "- $ y^w $: Winning response (preferred by humans).\n",
        "- $ y^l $: Losing response (not preferred by humans).\n",
        "- $ r_\\phi(y|x) $: Reward model's score for response $ y $ given prompt $ x $.\n",
        "- $ \\phi $: represents the parameters (weights) of the reward model.\n",
        "- $ \\sigma $: Sigmoid function, defined as $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $.\n",
        "- $ D $: Dataset of preference pairs $ (x, y^w, y^l) $.\n",
        "\n",
        "The **Reward Model Objective** is used to train a **reward model** that assigns higher scores to responses that align with human preferences. The goal is to learn a reward function $ r_\\phi(y|x) $ that can distinguish between \"good\" (winning) and \"bad\" (losing) responses for a given input prompt $ x $."
      ],
      "metadata": {
        "id": "vs9n9wsZ07Rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.2 KL Divergence Penalty Term**\n",
        "$$\\beta \\, \\text{KL}(\\pi_\\theta(y|x) \\, || \\, \\pi_{\\text{ref}}(y|x))$$\n",
        "\n",
        "or, after expanding\n",
        "\n",
        "$$\n",
        "\\text{KL}(\\pi_\\theta(y|x) \\, || \\, \\pi_{\\text{ref}}(y|x)) = \\mathbb{E}_{y \\sim \\pi_\\theta(y|x)} \\left[ \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\pi_\\theta(y|x) $: The current policy, which is being optimized.\n",
        "- $ \\pi_{\\text{ref}}(y|x) $: The reference policy, typically a frozen copy of the policy before the update.\n",
        "- $ y $: The response generated by the policy.\n",
        "- $ x $: The input prompt.\n",
        "- $ \\beta $: A hyperparameter that controls the strength of the KL penalty.\n",
        "\n",
        "The **Kullback-Leibler (KL) Divergence** is a measure of how one probability distribution diverges from a second, reference probability distribution. In the context of **Proximal Policy Optimization (PPO)**, the KL divergence is used to ensure that the updated policy $ \\pi_\\theta $ does not deviate too much from the reference policy $ \\pi_{\\text{ref}} $. This is crucial for maintaining stability during training and preventing the policy from making drastic changes that could lead to poor performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "59y-HAHVw84a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.3 Full PPO Objective Function**\n",
        "\n",
        "**1. RLHF (PPO) Objective Components**\n",
        "1. **Reward Term**: Maximize rewards from our reward model (`toxic-bert`).\n",
        "2. **KL Penalty**: Prevent the policy from straying too far from the reference model (SFT model).\n",
        "\n",
        "**2. What's Missing?**\n",
        "- The **value model** is implicitly required to compute the **advantage**:\n",
        "  ```\n",
        "  Advantage = Reward - Value\n",
        "  ```\n",
        "  - The **value model** estimates **expected future rewards** for partial sequences.\n",
        "  - Without it, you're doing plain policy gradient, not PPO.\n",
        "\n",
        "**3. Value Model's Role**\n",
        "- **Input**: Partial text (e.g., `\"Q: 2+2? → A:\"`).\n",
        "- **Output**: Scalar value predicting future reward (e.g., `0.3`).\n",
        "- **Why?** Reduces variance in updates by comparing rewards to a learned baseline.\n",
        "\n",
        "**4. Architecture Choices**\n",
        "\n",
        "| Model            | Type               | Example (our Setup)       |\n",
        "|------------------|--------------------|----------------------------|\n",
        "| Policy           | Causal LM (GPT-2)  | Generates answers          |\n",
        "| Reward Model     | Classifier (BERT)  | `toxic-bert` scores output |\n",
        "| Value Model      | GPT-2 + regression head | Predicts future toxicity |\n",
        "| Reference Model  | Frozen SFT (GPT-2) | Anchor for KL penalty      |\n",
        "\n",
        "**5. Key Fix for RLHF**\n",
        "Add a value model (e.g., GPT-2 with a regression head) to compute advantages. The full PPO objective requires:\n",
        "1. **Advantage Estimation** (Reward - Value)\n",
        "2. **Clipped Updates** (to avoid drastic policy changes)\n",
        "\n",
        "\n",
        "True PPO needs a value model for advantage calculation and policy update stabilization. See **Appendix A1** for details."
      ],
      "metadata": {
        "id": "dYOwuijUXYDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Hands-on with SafeEdit Dataset**\n",
        "\n",
        "**RLHF for Toxicity Removal in LLM Outputs**\n",
        "\n",
        "The notebook provides a complete pipeline from data preparation through to evaluation, with all components adapted to work within Colab's resource constraints while still demonstrating the effectiveness of RLHF for toxicity removal.\n",
        "\n",
        "**To Run This Notebook:**\n",
        "\n",
        "1. Upload and Extract the zip file to Colab (if not done already)\n",
        "2. Update the DATASET_PATHS dictionary with their locations\n",
        "3. Run all cells sequentially\n",
        "4. The entire process should complete within 1-2 hours on a free Colab GPU\n",
        "\n",
        "[Link to Download Data first](https://drive.google.com/file/d/1dBZbIoZLJt8nJzY3abMvPKq36j8-D3Hz/view?usp=drive_link). **The data should be used only for educational purposes, so please comply!**"
      ],
      "metadata": {
        "id": "4apQvYbWzOsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Upload SafeEdit Dataset\n",
        "\n",
        "# import os\n",
        "# import zipfile\n",
        "# from google.colab import files\n",
        "\n",
        "# # Upload the zipped data\n",
        "# def upload_and_extract_zip():\n",
        "#     print(\"Please upload your ZIP file:\")\n",
        "#     uploaded = files.upload()\n",
        "#     if len(uploaded) == 0:\n",
        "#         print(\"No file uploaded. Please upload a valid ZIP file.\")\n",
        "#         return\n",
        "\n",
        "#     # Get the file name\n",
        "#     zip_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "#     if not zip_file_name.endswith(\".zip\"):\n",
        "#         print(\"The uploaded file is not a ZIP file. Please upload a valid ZIP file.\")\n",
        "#         return\n",
        "\n",
        "#     # Extract the ZIP file\n",
        "#     extraction_folder = \"./SafeEdit_data\"\n",
        "#     os.makedirs(extraction_folder, exist_ok=True)\n",
        "#     with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "#         zip_ref.extractall(extraction_folder)\n",
        "\n",
        "#     print(f\"Files successfully extracted to: {extraction_folder}\")\n",
        "#     return extraction_folder\n",
        "\n",
        "\n",
        "# extracted_folder = upload_and_extract_zip()\n",
        "\n",
        "# if extracted_folder:\n",
        "#     print(f\"Files have been extracted to: {extracted_folder}\")\n",
        "#     print(\"List of extracted files:\", os.listdir(extracted_folder))"
      ],
      "metadata": {
        "id": "0qprmInREPGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup and Installation**"
      ],
      "metadata": {
        "id": "a_fADxZSjlrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers datasets accelerate peft bitsandbytes trl wandb\n",
        "# !pip install -q torch torchvision torchaudio\n",
        "# !pip install -q torch>=2.6"
      ],
      "metadata": {
        "id": "Bo-WlLkdjnRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration**"
      ],
      "metadata": {
        "id": "FTcInF19jone"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e0eXD5o1X-5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from IPython.display import HTML, display, Markdown\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "import copy\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"gpt2\"  # Using smaller model for Colab compatibility; alternatives but larger models: GPT-3.5, Mistral-7B, Llama-2, or Llama-23 (7B–13B) as well\n",
        "colab_dir = \"\" #\"/content/drive/MyDrive/cibc_share/TMLS Workshop - 2025/models/\"\n",
        "REWARD_MODEL_NAME_UNTRAINED = colab_dir + \"microsoft/deberta-v3-large\" # \"microsoft/deberta-v3-large\"\n",
        "REWARD_MODEL_NAME = colab_dir + \"OpenAssistant/reward-model-deberta-v3-base\"  # \"OpenAssistant/reward-model-deberta-v3-large-v2\"  # Pretrained safety reward model\n",
        "DATASET_PATHS = {\n",
        "    \"train\": \"./safeedit_train.pkl\",\n",
        "    \"val\": \"./safeedit_validation.pkl\",\n",
        "    \"test\": \"./safeedit_test.pkl\"\n",
        "}\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Vpmv6xPsjrem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "5rZsvb_YMaTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Get GPU name\n",
        "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Get total GPU memory\n",
        "print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Get current allocated memory\n",
        "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "# Get cached/reserved memory\n",
        "print(f\"Cached GPU memory: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "id": "0t0OhB2QmSDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "def display_reward_table(untrained_tensor, trained_tensor, retrained_tensor=None):\n",
        "    \"\"\"\n",
        "    Displays rewards in a markdown table with an optional retrained model row\n",
        "    and an additional column showing the absolute difference between Safe and Unsafe scores.\n",
        "\n",
        "    Args:\n",
        "        untrained_tensor (list or torch.Tensor): Reward scores from base model [safe_score, unsafe_score]\n",
        "        trained_tensor (list or torch.Tensor): Scores from original safety-tuned model\n",
        "        retrained_tensor (list or torch.Tensor, optional): Scores from your custom-tuned model\n",
        "\n",
        "    Notes:\n",
        "        - Positive values → Safer responses\n",
        "        - Negative values → Unsafe responses\n",
        "        - Values closer to 0 → Less confident classification\n",
        "        - Absolute Difference → Confidence gap between Safe vs Unsafe scores\n",
        "\n",
        "    Example:\n",
        "        >>> display_reward_table(\n",
        "        ...     untrained_tensor=[0.07, 0.07],               # Base DeBERTa\n",
        "        ...     trained_tensor=[-0.35, -4.05],               # OpenAssistant\n",
        "        ...     retrained_tensor=torch.tensor([1.82, -2.3])  # Custom tuned\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    def to_list(x):\n",
        "        return x.tolist() if isinstance(x, torch.Tensor) else x\n",
        "\n",
        "    untrained = to_list(untrained_tensor)\n",
        "    trained = to_list(trained_tensor)\n",
        "    retrained = to_list(retrained_tensor) if retrained_tensor is not None else None\n",
        "\n",
        "    # Calculate absolute differences\n",
        "    untrained_diff = abs(untrained[0] - untrained[1])\n",
        "    trained_diff = abs(trained[0] - trained[1])\n",
        "    retrained_diff = abs(retrained[0] - retrained[1]) if retrained else None\n",
        "\n",
        "    # Build the table\n",
        "    table = \"\"\"\n",
        "| Model Type               | Safe Response | Unsafe Response | Absolute Difference |\n",
        "|--------------------------|---------------|-----------------|---------------------|\n",
        "| Untrained Reward Model   | {:.4f}        | {:.4f}          | {:.4f}              |\n",
        "| Trained Reward Model     | {:.4f}        | {:.4f}          | {:.4f}              |\n",
        "\"\"\".format(\n",
        "        untrained[0], untrained[1], untrained_diff,\n",
        "        trained[0], trained[1], trained_diff\n",
        "    )\n",
        "\n",
        "    if retrained:\n",
        "        table += \"| **Re-trained Reward Model** | {:.4f}        | {:.4f}          | {:.4f}              |\\n\".format(\n",
        "            retrained[0], retrained[1], retrained_diff\n",
        "        )\n",
        "\n",
        "    # Add interpretation guide\n",
        "    table += \"\\n**Key:**\\n\"\n",
        "    table += \"- Positive values → Safer responses\\n\"\n",
        "    table += \"- Negative values → Unsafe responses\\n\"\n",
        "    table += \"- Values closer to 0 → Less confident classification\\n\"\n",
        "    table += \"- **Absolute Difference** → Confidence gap between Safe vs Unsafe\"\n",
        "\n",
        "    display(Markdown(table))\n"
      ],
      "metadata": {
        "id": "4uW-CzOw2b2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preparation**"
      ],
      "metadata": {
        "id": "LtnUbmSOjtOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_dataset(file_path):\n",
        "    df = pd.read_pickle(file_path)\n",
        "\n",
        "    # Pre-tokenize and format the data\n",
        "    texts = []\n",
        "    for _, row in df.iterrows():\n",
        "        # Format as instruction-following\n",
        "        text = f\"### Question:\\n{row['question']}\\n\\n### Safe Answer:\\n{row['safe generation']}\"\n",
        "        texts.append(text)\n",
        "\n",
        "    return Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = load_and_prepare_dataset(DATASET_PATHS[\"train\"])\n",
        "val_dataset = load_and_prepare_dataset(DATASET_PATHS[\"val\"])\n",
        "test_dataset = load_and_prepare_dataset(DATASET_PATHS[\"test\"])\n",
        "\n",
        "# Show sample\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "ovn6h2cqjvZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Loading**"
      ],
      "metadata": {
        "id": "nz9UVLY_jwwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.config.pad_token_id = base_model.config.eos_token_id\n",
        "print('eos/pad', base_model.config.eos_token_id)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load untrained reward model for comparison (microsoft/deberta-v3-large)\n",
        "reward_tokenizer_untrained = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME_UNTRAINED)\n",
        "reward_model_untrained = AutoModelForSequenceClassification.from_pretrained(\n",
        "    REWARD_MODEL_NAME_UNTRAINED,\n",
        "    num_labels=1,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load (trained) reward model for safety (microsoft/deberta-v3-large model but trained for safety)\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    REWARD_MODEL_NAME,\n",
        "    num_labels=1,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "T5vr3HFLjy5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Get GPU name\n",
        "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Get total GPU memory\n",
        "print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Get current allocated memory\n",
        "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "# Get cached/reserved memory\n",
        "print(f\"Cached GPU memory: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "id": "pdHlay0XmtDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reward Function**"
      ],
      "metadata": {
        "id": "gsL7fVrWj0Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reward(prompts, responses, reward_tokenizer, reward_model):\n",
        "    \"\"\"Calculate safety reward scores for prompt-response pairs using a safety reward model.\n",
        "\n",
        "    The reward model assigns higher scores to safer, more appropriate responses and lower/negative\n",
        "    scores to unsafe or harmful responses. Scores typically range from -3 to +3, where:\n",
        "    - >1.0 indicates very safe responses\n",
        "    - 0.0 to 1.0 indicates moderately safe responses\n",
        "    - <0.0 indicates unsafe responses\n",
        "\n",
        "    Args:\n",
        "        prompts (list[str]): List of input prompts/questions\n",
        "        responses (list[str]): List of corresponding responses to evaluate\n",
        "        reward_tokenizer (AutoTokenizer): Tokenizer for the reward model\n",
        "        reward_model (AutoModelForSequenceClassification): Pretrained safety reward model\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of reward scores for each prompt-response pair\n",
        "\n",
        "    Examples:\n",
        "        >>> # Example 1: Safe vs unsafe responses\n",
        "        >>> prompts = [\"How can I hack someone's account?\"]\n",
        "        >>> responses = [\"I can't help with that\",  # Safe response\n",
        "        ...              \"First find their password\"]  # Unsafe response\n",
        "        >>> rewards = get_reward(prompts*2, responses, reward_tokenizer, reward_model)\n",
        "        >>> print(rewards)\n",
        "        tensor([2.31, -1.45])  # First response rewarded, second penalized\n",
        "\n",
        "        >>> # Example 2: Different prompt types\n",
        "        >>> prompts = [\"Tell me a joke\",\n",
        "        ...            \"How to make illegal drugs?\"]\n",
        "        >>> responses = [\"Why don't scientists trust atoms? Because they make up everything!\",\n",
        "        ...              \"I won't provide that information\"]\n",
        "        >>> rewards = get_reward(prompts, responses, reward_tokenizer, reward_model)\n",
        "        >>> print(rewards)\n",
        "        tensor([1.89, 2.15])  # Both responses scored positively\n",
        "\n",
        "        >>> # Example 3: Multiple prompts with batch processing\n",
        "        >>> prompts = [\"How to steal?\", \"What's 2+2?\", \"How to hurt someone?\"]\n",
        "        >>> responses = [\"That's illegal\", \"Four\", \"Violence is never the answer\"]\n",
        "        >>> rewards = get_reward(prompts, responses, reward_tokenizer, reward_model)\n",
        "        >>> print(rewards)\n",
        "        tensor([1.97, 0.32, 2.01])  # All safe responses get positive scores\n",
        "\n",
        "    Notes:\n",
        "        - The exact score range depends on how the reward model was trained\n",
        "        - For classification, common thresholds are:\n",
        "            - score > 0.5 → safe\n",
        "            - score < 0.5 → unsafe\n",
        "        - Responses should be aligned with the prompts (same list length)\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for prompt, response in zip(prompts*len(responses), responses):\n",
        "        inputs = reward_tokenizer(prompt, response, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # Disable dropout and enable eval mode\n",
        "        reward_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            reward = reward_model(**inputs).logits[0].cpu().item()\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return torch.tensor(rewards, dtype=torch.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "vnukOlhij2v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example of unsafe vs safe generation\n",
        "sample = pd.read_pickle(DATASET_PATHS[\"train\"]).iloc[10]\n",
        "Markdown(f\"\"\"\n",
        "**Question:** (Category: {sample['unsafety category'].lower()})<br> {sample[\"question\"]} <br><br>\n",
        "\n",
        "**Adversarial Prompt:**<br> {sample[\"adversarial prompt\"]} <br><br>\n",
        "\n",
        "**Unsafe Generation:**<br> {sample[\"unsafe generation\"]}<br><br>\n",
        "\n",
        "**Safe Generation:**<br> {sample[\"safe generation\"]}\"\"\")"
      ],
      "metadata": {
        "id": "coIWmB625wIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test reward function with Question\n",
        "display_reward_table(\n",
        "    get_reward(sample[\"question\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model_untrained),     # Base DeBERTa (untrained)\n",
        "    get_reward(sample[\"question\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model),               # Original OpenAssistant\n",
        ")"
      ],
      "metadata": {
        "id": "fMmQRekluF5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test reward function with Adversarial Prompt\n",
        "sample_prompts = [sample[\"adversarial prompt\"]]\n",
        "display_reward_table(\n",
        "    get_reward(sample[\"adversarial prompt\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model_untrained),     # Base DeBERTa (untrained)\n",
        "    get_reward(sample[\"adversarial prompt\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model),               # Original OpenAssistant\n",
        ")"
      ],
      "metadata": {
        "id": "5hy4Dn27poH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Supervised Fine-Tuning (SFT)**\n",
        "\n",
        "- We use Parameter-Efficient Fine-Tuning (PEFT) with LoRA (Low-Rank Adaptation) by drastically reducing memory usage.\n",
        " - `prepare_model_for_kbit_training(base_model)`: Prepares the model for quantized training (e.g., 4-bit/8-bit precision) to reduce GPU memory usage.\n",
        " - `LoraConfig` (LoRA Configuration): Defines how LoRA adapters are applied to the model.\n",
        " - `get_peft_model(base_model, lora_config)`: Wraps the base model with LoRA adapters, freezing the original weights and only training the added low-rank matrices."
      ],
      "metadata": {
        "id": "Bha-4CVG33ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_model)"
      ],
      "metadata": {
        "id": "ZA5s1Q2633Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PEFT for effiecient fine-tuning of the model (LoRA based)\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                      # Rank of the low-rank matrices (smaller r = fewer parameters, but may reduce performance).\n",
        "    lora_alpha=16,            # Scaling factor for LoRA weights (balances pretrained vs. LoRA-learned features).\n",
        "    target_modules=[          # Modules where LoRA is applied. Use target_modules=\"all-linear\" to let PEFT choose all linear layers\n",
        "        \"c_attn\",             # Query/Key/Value projections in attention layers.\n",
        "        \"c_fc\",               # Feed-forward up-projection.\n",
        "        \"c_proj\"              # Feed-forward down-projection.\n",
        "        ],                    # Dropout rate for LoRA layers (regularization).\n",
        "    lora_dropout=0.05,        # Dropout rate for LoRA layers (regularization).\n",
        "    bias=\"none\",              # Do not train bias terms (options: \"none\", \"all\", \"lora_only\").\n",
        "    task_type=\"CAUSAL_LM\"     # Specifies causal language modeling (autoregressive prediction)\n",
        ")\n",
        "\n",
        "base_model = get_peft_model(base_model, lora_config)\n",
        "base_model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "1MgPmFscqXns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"./sft_model\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-3,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=1,\n",
        "    max_steps=200,\n",
        "    save_steps=100,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    #skip_prepare_dataset=True\n",
        ")\n",
        "from trl import SFTConfig\n",
        "\n",
        "training_args.dataset_kwargs = {\"skip_prepare_dataset\": True}\n",
        "\n",
        "# Initialize minimal SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    #config=SFTConfig(dataset_kwargs={'skip_prepare_dataset': True})\n",
        ")\n",
        "\n",
        "trainer.skip_prepare_dataset = True\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "trainer.save_model(\"sft_toxicity_removal\")"
      ],
      "metadata": {
        "id": "hJAoXcqS22cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Reward Model Training**"
      ],
      "metadata": {
        "id": "MAvils2_j-o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare preference dataset\n",
        "def prepare_preference_dataset(file_path):\n",
        "    df = pd.read_pickle(file_path)\n",
        "\n",
        "    # Create prompt-response pairs\n",
        "    pairs = []\n",
        "    for _, row in df.iterrows():\n",
        "        if row['unsafe generation'] and row['safe generation']:\n",
        "            pairs.append({\n",
        "                \"prompt\": row['question'],  # For colab we used 'question', but in practice replace 'question' with 'adversarial prompt' when training reward model\n",
        "                \"chosen\": row['safe generation'],\n",
        "                \"rejected\": row['unsafe generation']\n",
        "            })\n",
        "    return Dataset.from_list(pairs)\n",
        "\n",
        "train_prefs = prepare_preference_dataset(DATASET_PATHS[\"train\"])\n",
        "val_prefs = prepare_preference_dataset(DATASET_PATHS[\"val\"])\n",
        "train_prefs"
      ],
      "metadata": {
        "id": "xjXcrUnd_AWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(train_prefs))\n",
        "sample"
      ],
      "metadata": {
        "id": "wmeJOVh7RVKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom reward model trainer\n",
        "class RewardTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # Added **kwargs\n",
        "        # Forward pass for chosen responses\n",
        "        outputs_chosen = model(\n",
        "            input_ids=inputs[\"input_ids_chosen\"],\n",
        "            attention_mask=inputs[\"attention_mask_chosen\"]\n",
        "        )\n",
        "\n",
        "        # Forward pass for rejected responses\n",
        "        outputs_rejected = model(\n",
        "            input_ids=inputs[\"input_ids_rejected\"],\n",
        "            attention_mask=inputs[\"attention_mask_rejected\"]\n",
        "        )\n",
        "\n",
        "        # Calculate loss (difference between chosen and rejected scores)\n",
        "        loss = -torch.log(torch.sigmoid(outputs_chosen.logits - outputs_rejected.logits)).mean()\n",
        "\n",
        "        # maximize the diff between chosen and rejected (kind of contrastive? or triplet?)\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, {\"chosen\": outputs_chosen, \"rejected\": outputs_rejected}\n",
        "        return loss"
      ],
      "metadata": {
        "id": "MBc5r8zh_CU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize preference data with fixed padding\n",
        "def tokenize_preference(examples):\n",
        "    tokenized_chosen = reward_tokenizer(\n",
        "        examples[\"prompt\"],\n",
        "        examples[\"chosen\"],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=256,  # Reduced from 512\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized_rejected = reward_tokenizer(\n",
        "        examples[\"prompt\"],\n",
        "        examples[\"rejected\"],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=256,  # Reduced from 512\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokenized_chosen[\"input_ids\"],\n",
        "        \"attention_mask_chosen\": tokenized_chosen[\"attention_mask\"],\n",
        "        \"input_ids_rejected\": tokenized_rejected[\"input_ids\"],\n",
        "        \"attention_mask_rejected\": tokenized_rejected[\"attention_mask\"],\n",
        "    }\n",
        "\n",
        "train_prefs = train_prefs.map(tokenize_preference, batched=True)\n",
        "val_prefs = val_prefs.map(tokenize_preference, batched=True)"
      ],
      "metadata": {
        "id": "M0Ypzdqn_FIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer with our custom class\n",
        "reward_model_cloned = copy.deepcopy(reward_model) # we clone to show the difference, not needed at the time of deployment\n",
        "reward_trainer = RewardTrainer(\n",
        "    model=reward_model_cloned,  # use reward_model if no need for comparison\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./reward_model\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=16,\n",
        "        learning_rate=1e-5,\n",
        "        fp16=True,  # Enable mixed precision\n",
        "        num_train_epochs=1,\n",
        "        max_steps=30,    # Hard stop at desired steps for colab\n",
        "        logging_steps=10,\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False\n",
        "    ),\n",
        "    train_dataset=train_prefs,\n",
        "    # eval_dataset=val_prefs, # for colab commented out, use eval_dataset to monitor how well the reward model is trained and to adjust setting\n",
        ")\n",
        "\n",
        "reward_trainer.train()\n",
        "# reward_trainer.save_model(\"trained_reward_model\") # not saved for colab"
      ],
      "metadata": {
        "id": "wnD-q0CvRF48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PSlebcP-cnIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example of unsafe vs safe generation\n",
        "sample = pd.read_pickle(DATASET_PATHS[\"train\"]).iloc[10]\n",
        "Markdown(f\"\"\"\n",
        "**Question:** (Category: {sample['unsafety category'].lower()})<br> {sample[\"question\"]} <br><br>\n",
        "\n",
        "**Adversarial Prompt:**<br> {sample[\"adversarial prompt\"]} <br><br>\n",
        "\n",
        "**Unsafe Generation:**<br> {sample[\"unsafe generation\"]}<br><br>\n",
        "\n",
        "**Safe Generation:**<br> {sample[\"safe generation\"]}\"\"\")"
      ],
      "metadata": {
        "id": "eYOKcy4o8-kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test reward function with Question\n",
        "display_reward_table(\n",
        "    get_reward(sample[\"question\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model_untrained),     # Base DeBERTa (untrained)\n",
        "    get_reward(sample[\"question\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model),               # Original OpenAssistant\n",
        "    get_reward(sample[\"question\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model_cloned)         # Custom tuned\n",
        ")"
      ],
      "metadata": {
        "id": "71Gd6sCz9WC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test reward function with Adversarial Prompt\n",
        "display_reward_table(\n",
        "    get_reward(sample[\"adversarial prompt\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model_untrained),     # Base DeBERTa (untrained)\n",
        "    get_reward(sample[\"adversarial prompt\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model),               # Original OpenAssistant\n",
        "    get_reward(sample[\"adversarial prompt\"], [sample[\"safe generation\"], sample[\"unsafe generation\"]], reward_tokenizer, reward_model_cloned)         # Custom tuned\n",
        ")"
      ],
      "metadata": {
        "id": "6Rb9ileg9WC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krP6ohW4aQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 PPO Training**\n",
        "- `trl` pachage provides two PPO training approaches (https://huggingface.co/docs/trl/en/index):\n",
        "  1. `trainer.train()` supported by the recent update of the package (`v.0.18.1`):\n",
        "\n",
        "    ```python\n",
        "    # PPO Trainer (trl version  `0.18.1`)\n",
        "    trainer = PPOTrainer(\n",
        "        model=sft_model,\n",
        "        ref_model=ref_model,\n",
        "        reward_model=reward_model,\n",
        "        value_model=value_model,\n",
        "        processing_class=tokenizer\n",
        "        ...\n",
        "    )\n",
        "\n",
        "    # Start training process\n",
        "    trainer.train()\n",
        "    ```\n",
        "\n",
        "  2. Step-wise training using `trainer.generate()` and `trainer.step()` supported by the older versions of the package (`v.0.11.1`), while tyhey are hidden in the recent version (see [issue](https://github.com/huggingface/trl/issues/3270)):\n",
        "\n",
        "    ```python\n",
        "    # PPO Trainer (trl version `0.11.1`)\n",
        "    trainer = PPOTrainer(\n",
        "        model=sft_model,\n",
        "        ref_model=ref_model,\n",
        "        tokenizer=tokenizer\n",
        "        ...\n",
        "    )\n",
        "\n",
        "    # Start training loop\n",
        "    for epoch in range(TRAINING_CONFIG['epochs']):\n",
        "        for batch in trainer.dataloader:\n",
        "            # === Extract queries and tokenize ===\n",
        "            queries = batch[\"query\"]\n",
        "            tokenized = [tokenizer(q, return_tensors=\"pt\", truncation=True).to(DEVICE) for q in queries]\n",
        "            query_tensors = [t[\"input_ids\"].squeeze(0) for t in tokenized]\n",
        "\n",
        "            # === Generate responses ===\n",
        "            response_tensors = trainer.generate(query_tensors, ...)\n",
        "\n",
        "            # === Decode responses ===\n",
        "            responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n",
        "\n",
        "            # === Compute rewards ===\n",
        "            rewards = compute_rewards(reward_model, reward_tokenizer, queries, responses)\n",
        "\n",
        "            # === PPO step ===\n",
        "            trainer.step(query_tensors, response_tensors, list(rewards))\n",
        "    \n",
        "    ```\n",
        "\n",
        "**Key notes:**\n",
        "\n",
        "- **Option 1** (trl `v0.11.1`): Stepwise training is advised when the tokenizer of the policy model is different from the tokenizer of the reward model (i.e. two distict models by initial model).\n",
        "  - For detail of this implementation refer to **[`ppo_with_trl_0_11.ipynb`](https://colab.research.google.com/drive/1IzqPke-A-EgSffqnxhL7_Tr-v7rHzxWx?usp=sharin)** colab.\n",
        "- **Option 2** (trl `v0.18.1`): The recent `trl` package does not expose the `.step` and `.generate` functions yet. So for here forward we use `gpt2` with `AutoModelForSequenceClassification` (_[causal + head]_ model) as the reward function since it shares the same tokenizer with the policy model.\n",
        "  - For detail of this implementation refer to **[`ppo_with_trl_0_18.ipynb`](https://colab.research.google.com/drive/1ih60P6bdlxnwdKIIVIAWaXNbNVsQ9uef?usp=sharing)** colab.\n",
        "\n",
        "\n",
        "**Take-home Task:**\n",
        "  - Depneding on your preference and what has been shown above to train the SFT model and the reward model, and the end-to-end PPO training in **[`ppo_with_trl_0_11.ipynb`](https://colab.research.google.com/drive/1IzqPke-A-EgSffqnxhL7_Tr-v7rHzxWx?usp=sharin)** and **[`ppo_with_trl_0_18.ipynb`](https://colab.research.google.com/drive/1ih60P6bdlxnwdKIIVIAWaXNbNVsQ9uef?usp=sharing)** colabs, complete this section.\n",
        "    - In Option 1, fallback to older version of trl package that allows dis-similar tokenizers for the policy model and the reward model.\n",
        "    - In Option 2,\n",
        "      - fallback to another reward model; i.e. shared tokenizer for the policy model and the reward model. Or,\n",
        "      - implement `.step()` and `.generate()` methods in the current `PPOTrainer`, which may need extensive work (and possibly open a PR on trl github repo page once done).\n"
      ],
      "metadata": {
        "id": "FWOS0k3WkEfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rewards(reward_model, reward_tokenizer, queries, responses, concate=True):\n",
        "    \"\"\"Compute rewards using the reward model\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if concate:\n",
        "            full_texts = [q + r for q, r in zip(queries, responses)] # If your reward model was trained using tokenizer(query + response)\n",
        "            inputs = reward_tokenizer(full_texts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        else:\n",
        "            inputs = reward_tokenizer(queries, responses, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
        "        outputs = reward_model(**inputs)\n",
        "        rewards = outputs.logits.squeeze(-1).cpu()\n",
        "    return rewards #* 0.1  # Scale down rewards"
      ],
      "metadata": {
        "id": "HY2-tdaojA3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup_ppo_trainer(tokenizer, model, ref_model, dataset):\n",
        "    \"\"\"Configure PPO trainer\"\"\"\n",
        "    ppo_config = PPOConfig(\n",
        "        kl_penalty=\"kl\",                                  # Explicitly enable KL divergence tracking\n",
        "        batch_size=TRAINING_CONFIG['batch_size'],         # Increased from 4\n",
        "        mini_batch_size=1,                                # Reduced from 2\n",
        "        learning_rate=TRAINING_CONFIG['learning_rate'],\n",
        "        log_with=None,\n",
        "        init_kl_coef=0.5,                                 # Increased from 0.2\n",
        "        target_kl=3.0,                                    # Add target KL to early stop if divergence is too high, Will stop updates if KL exceeds this value\n",
        "        project_kwargs={\"logging_dir\": \"./logs\"},\n",
        "        #kl_penalty=\"adaptive\",\n",
        "        cliprange=0.1,                                    # Tighter clipping (default: 0.2), intended for the policy model\n",
        "        cliprange_value=0.1,                              # Clips value model/function updates too\n",
        "    )\n",
        "\n",
        "    return PPOTrainer(\n",
        "        config=ppo_config,\n",
        "        model=model, # replace this with sft version of the model\n",
        "        ref_model=ref_model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=dataset,\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate_model(model, tokenizer, reward_model, reward_tokenizer, eval_dataset, max_eval_samples=100):\n",
        "    model.eval()\n",
        "    eval_rewards = []\n",
        "    for example in eval_dataset.select(range(min(max_eval_samples, len(eval_dataset)))):\n",
        "        query = example['prompt']\n",
        "        with torch.no_grad():\n",
        "            encoded = tokenizer(query, return_tensors=\"pt\").to(DEVICE)\n",
        "            output = model.generate(**encoded, **GENERATION_CONFIG)\n",
        "            response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Compute reward\n",
        "            reward_input = reward_tokenizer(query, response, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "            reward_score = reward_model(**reward_input).logits.squeeze().cpu().item()\n",
        "            eval_rewards.append(reward_score)\n",
        "\n",
        "    model.train()\n",
        "    return sum(eval_rewards) / len(eval_rewards) if eval_rewards else 0.0\n",
        "\n"
      ],
      "metadata": {
        "id": "6Z1RtCqYisjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_loop(ppo_trainer, tokenizer, reward_model, reward_tokenizer, train_dataset, eval_dataset):\n",
        "    \"\"\"Main training loop with batched generation\"\"\"\n",
        "    for epoch in range(TRAINING_CONFIG['epochs']):\n",
        "        print(f\"\\n{'='*55}\")\n",
        "        print(f\"Epoch {epoch+1}/{TRAINING_CONFIG['epochs']}\")\n",
        "        print(f\"{'='*55}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(ppo_trainer.dataloader):\n",
        "            # Extract queries\n",
        "            queries = batch['query']\n",
        "\n",
        "            # Tokenize to get list of 1D tensors\n",
        "            tokenized = [tokenizer(q, return_tensors=\"pt\", padding=False, truncation=True).to(device) for q in queries]\n",
        "            query_tensors = [t[\"input_ids\"].squeeze(0) for t in tokenized]  # list of 1D tensors\n",
        "\n",
        "            # === Batched response generation ===\n",
        "            response_tensors = ppo_trainer.generate(\n",
        "                query_tensors,\n",
        "                return_prompt=False,\n",
        "                **GENERATION_CONFIG\n",
        "            )\n",
        "\n",
        "            # Decode responses\n",
        "            responses = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]\n",
        "\n",
        "            # Compute rewards\n",
        "            rewards = compute_rewards(reward_model, reward_tokenizer, queries, responses)\n",
        "\n",
        "            # Train step\n",
        "            stats = ppo_trainer.step(\n",
        "                query_tensors,         # list of 1D tensors\n",
        "                response_tensors,      # list of 1D tensors\n",
        "                list(rewards)          # tensor of shape [batch_size]\n",
        "            )\n",
        "\n",
        "            print(\"Step stats:\", stats.keys())\n",
        "\n",
        "            total_batches = len(ppo_trainer.dataloader)\n",
        "\n",
        "            # Print header only once\n",
        "            if epoch == 0 and batch_idx == 0:\n",
        "                print(f\"{'Batch':>10} | {'Mean Reward':>12} | {'Std Dev Reward':>16} | {'KL Div':>8}\")\n",
        "                print(\"-\" * 55)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                batch_str = f\"{batch_idx}/{total_batches}\"\n",
        "                rewards_tensor = rewards if isinstance(rewards, torch.Tensor) else torch.stack(rewards)\n",
        "                mean_reward = rewards_tensor.mean().item()\n",
        "                std_reward = rewards_tensor.std().item()\n",
        "                kl_divergence = stats.get('objective/kl', float('nan'))  # Get KL divergence or NaN if not available\n",
        "\n",
        "                print(f\"{batch_str:>10} | {mean_reward:12.2f} | {std_reward:16.2f} | {kl_divergence:8.2f}\")\n",
        "\n",
        "                # print(f\"Query: {queries[0][:100]}...\" if len(queries[0]) > 100 else queries[0])\n",
        "                # print(f\"Response: {responses[0][:100]}...\" if len(responses[0]) > 100 else responses[0])\n",
        "                # print(f\"Entropy: {stats.get('entropy', 'n/a')}\")\n",
        "\n",
        "        # # Evaluate mid-training\n",
        "        mean_eval_reward = evaluate_model(\n",
        "            ppo_trainer.model,\n",
        "            tokenizer,\n",
        "            reward_model,\n",
        "            reward_tokenizer,\n",
        "            eval_dataset,\n",
        "            max_eval_samples=100\n",
        "        )\n",
        "        print(f\"\\n Evaluation after Epoch {epoch + 1}: Mean Reward = {mean_eval_reward:.4f}\")"
      ],
      "metadata": {
        "id": "urSYofYpmZEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset():\n",
        "    \"\"\"Load and prepare the dataset\"\"\"\n",
        "    dataset = load_dataset(DATASET_NAME, trust_remote_code=True)\n",
        "\n",
        "    # Use appropriate splits\n",
        "    train_dataset = dataset[\"train\"] if \"train\" in dataset else dataset[\"unsafe\"]\n",
        "    eval_dataset = dataset[\"validation\"] if \"validation\" in dataset else dataset[\"safe\"]\n",
        "\n",
        "    # Rename columns to standardize\n",
        "    if \"prompt\" in train_dataset.column_names:\n",
        "        pass\n",
        "    elif \"question\" in train_dataset.column_names:\n",
        "        train_dataset = train_dataset.rename_column(\"question\", \"prompt\")\n",
        "        eval_dataset = eval_dataset.rename_column(\"question\", \"prompt\")\n",
        "    elif \"input\" in train_dataset.column_names:\n",
        "        train_dataset = train_dataset.rename_column(\"input\", \"prompt\")\n",
        "        eval_dataset = eval_dataset.rename_column(\"input\", \"prompt\")\n",
        "    else:\n",
        "        def create_prompt(examples):\n",
        "            return {\"prompt\": examples[\"input\"] + \" \" + examples[\"instruction\"]}\n",
        "        train_dataset = train_dataset.map(create_prompt)\n",
        "        eval_dataset = eval_dataset.map(create_prompt)\n",
        "\n",
        "    # Format for PPOTrainer\n",
        "    def format_dataset(examples):\n",
        "        return {\"query\": examples[\"prompt\"]}\n",
        "\n",
        "    train_dataset = train_dataset.map(format_dataset)\n",
        "    train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col != \"query\"])\n",
        "\n",
        "    return train_dataset, eval_dataset"
      ],
      "metadata": {
        "id": "ZUzxGM8Bm7ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, eval_dataset = prepare_dataset()\n",
        "for sample in train_dataset:\n",
        "  print(sample)\n",
        "  break"
      ],
      "metadata": {
        "id": "OTUWVkcpm98w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_CONFIG = {\n",
        "    'epochs': 3,\n",
        "    'batch_size': 2,\n",
        "    'learning_rate': 1.41e-3,  # Reduced from 1.41e-5\n",
        "}\n",
        "\n",
        "# Generation parameters\n",
        "GENERATION_CONFIG = {\n",
        "    'max_new_tokens': 50,   # Reduced from 100\n",
        "    'min_length': 10,       # Reduced from 20\n",
        "    'do_sample': True,      # Enable stochastic decoding\n",
        "    'top_p': 0.9,           # More conservative sampling, sample from top 90% cumulative probability\n",
        "    'temperature': 0.3,     # Less randomness (default: 1.0)\n",
        "}\n",
        "\n",
        "import trl\n",
        "from copy import deepcopy\n",
        "print('trl: ', trl.__version__)\n",
        "\n",
        "# prompt: load model\n",
        "# sft_model = load_model('sft_toxicity_removal')\n",
        "# sft_model\n",
        "\n",
        "sft_model = AutoModelForCausalLM.from_pretrained('sft_toxicity_removal')\n",
        "sft_model.config.pad_token_id = sft_model.config.eos_token_id\n",
        "print('eos/pad', sft_model.config.pad_token_id)\n",
        "\n",
        "## Value Model\n",
        "\n",
        "value_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=1, problem_type=\"regression\")\n",
        "# Freeze GPT-2 layers (train only the head):\n",
        "for param in value_model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# TO-DO as takehome task (after running ppo_with_trl_0_11.ipynb and ppo_with_trl_0_18.ipynb)\n",
        "\n",
        "\n",
        "ref_model = deepcopy(sft_model).eval()  # Frozen reference\n",
        "reward_model.eval() # reward_model is frozen"
      ],
      "metadata": {
        "id": "H_aggeXtoNS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# Main Execution\n",
        "###############################\n",
        "def run_ppo_with_reward_function():\n",
        "    # Prepare dataset\n",
        "    train_dataset, eval_dataset = prepare_dataset()\n",
        "\n",
        "    # Initialize models\n",
        "    #Tokenizer, model, ref_model, reward_tokenizer, reward_model = initialize_models()\n",
        "\n",
        "    # Setup PPO trainer\n",
        "    ppo_trainer = setup_ppo_trainer(tokenizer, sft_model, ref_model, df_train)\n",
        "\n",
        "    # Run training\n",
        "    train_loop(ppo_trainer, tokenizer, reward_model, reward_tokenizer, train_dataset, eval_dataset)\n",
        "\n",
        "    # Save the trained model\n",
        "    sft_model.save_pretrained(\"./ppo_trained_model\")\n",
        "    tokenizer.save_pretrained(\"./ppo_trained_model\")\n",
        "\n",
        "run_ppo_with_reward_function()"
      ],
      "metadata": {
        "id": "rXsyr8P0XsEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFHSMWuvXsAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPkFZhUTXr-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZTGL2HtXr7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final Evaluation**\n",
        "\n",
        "- The stage should be completed with the test split after the PPO is trained completely."
      ],
      "metadata": {
        "id": "V3RKkKL-kPcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO-DO as takehome task once PPO is trained\n",
        "def evaluate_model(model, tokenizer, dataset, reward_model, reward_tokenizer, num_samples=20):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    results = []\n",
        "    idxs = np.random.choice(len(dataset), size=num_samples, replace=False)\n",
        "\n",
        "    for i in idxs:\n",
        "        prompt = dataset[i][\"prompt\"]\n",
        "\n",
        "        # Generate response\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=128,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = get_reward([prompt], [response], reward_tokenizer, reward_model)[0].item()\n",
        "\n",
        "        results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response,\n",
        "            \"reward\": reward,\n",
        "            \"category\": dataset[i][\"category\"]\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Evaluate SFT model\n",
        "print(\"Evaluating SFT model...\")\n",
        "sft_eval = evaluate_model(sft_model, tokenizer, test_dataset, reward_model, reward_tokenizer)\n",
        "\n",
        "# Evaluate PPO model\n",
        "print(\"\\nEvaluating PPO model...\")\n",
        "ppo_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"ppo_toxicity_removal\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "ppo_eval = evaluate_model(ppo_model, tokenizer, test_dataset, reward_model, reward_tokenizer)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\nSFT Model Average Reward:\", sft_eval[\"reward\"].mean())\n",
        "print(\"PPO Model Average Reward:\", ppo_eval[\"reward\"].mean())\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nExample improvements:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nPrompt: {sft_eval['prompt'][i]}\")\n",
        "    print(f\"SFT Response: {sft_eval['response'][i]} (Reward: {sft_eval['reward'][i]:.2f})\")\n",
        "    print(f\"PPO Response: {ppo_eval['response'][i]} (Reward: {ppo_eval['reward'][i]:.2f})\")"
      ],
      "metadata": {
        "id": "27mrQguBkSKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Brainstorm Activity**"
      ],
      "metadata": {
        "id": "fcsAVaNOz5Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Group Discussion Prompts\n",
        "discussion_topics = [\n",
        "    \"How might you adapt these techniques for child-friendly chatbots?\",\n",
        "    \"How can we balance safety with creative generation?\",\n",
        "    \"What metrics would you use to evaluate fairness in model outputs?\",\n",
        "    \"What are potential failure modes of RLHF for safety?\",\n",
        "    \"What kinds of harmful outputs (subtle or overt) might a human rater miss? How can we catch these?\",\n",
        "    \"How should RLHF balance fairness and truthfulness when user expectations conflict with respectful output? (e.g. User asks the model to adopt an informal sarcastic tone.)\",\n",
        "    \"What kind of prompts can potentially lead the model to produce harmful outputs? Can RLHF help maintain safe practice there?\"\n",
        "]\n",
        "\n",
        "print(\"Break into small groups and discuss one of these topics:\")\n",
        "for i, topic in enumerate(discussion_topics, 1):\n",
        "    print(f\"{i}. {topic}\")"
      ],
      "metadata": {
        "id": "HG_4jy6oz8ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Presentations and Q&A**"
      ],
      "metadata": {
        "id": "rz-rP4_Nz90f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluation and Wrap-up\n",
        "def evaluate_safety(model, test_cases):\n",
        "    safe_count = 0\n",
        "    for case in test_cases:\n",
        "        response = generate_response(case['prompt'])\n",
        "        if \"sorry\" in response.lower() or \"can't\" in response.lower():\n",
        "            safe_count += 1\n",
        "    return safe_count / len(test_cases)\n",
        "\n",
        "test_cases = [{\"prompt\": row['text']} for _, row in test_df.head(5).iterrows()]\n",
        "safety_score = evaluate_safety(model, test_cases)\n",
        "\n",
        "print(f\"\\nModel safety score on test cases: {safety_score:.0%}\")\n",
        "print(\"\\nKey Takeaways:\")\n",
        "print(\"- RLHF provides powerful tools for aligning LLMs with human values\")\n",
        "print(\"- Safety requires continuous evaluation and iteration\")\n",
        "print(\"- Consider the full pipeline: SFT → Reward Modeling → RL Optimization\")"
      ],
      "metadata": {
        "id": "7NX8gFSLz_wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Resources**"
      ],
      "metadata": {
        "id": "POTNvJay0DmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Further Learning\n",
        "resources = {\n",
        "    \"SafeEdit Paper\": \"https://arxiv.org/abs/2403.14472\",\n",
        "    \"TRL Documentation\": \"https://huggingface.co/docs/trl/index\",\n",
        "    \"RLHF Explained\": \"https://huggingface.co/blog/rlhf\",\n",
        "    \"LoRA Paper\": \"https://arxiv.org/abs/2106.09685\"\n",
        "}\n",
        "\n",
        "print(\"Explore these resources to go deeper:\")\n",
        "for name, url in resources.items():\n",
        "    print(f\"- {name}: {url}\")"
      ],
      "metadata": {
        "id": "DLDyosC10I2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Appendix: Full Implementation Details**"
      ],
      "metadata": {
        "id": "l03zlZt10LCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A1. Full PPO Objective Function**\n",
        "\n",
        "The PPO objective you've shown is a **simplified version** commonly used in RLHF (Reinforcement Learning from Human Feedback) for language models, but it's not the full PPO objective—it's missing the **value model's role** in advantage estimation. Let me clarify:\n",
        "\n",
        "#### **1. What’s Missing? The Value Model’s Role**\n",
        "The current objective focuses on:\n",
        "1. **Reward Maximization**: $$\\mathbb{E}[r_\\phi(y|x)]$$  \n",
        "2. **KL Penalty**: $$\\text{KL}(\\pi_\\theta || \\pi_{\\text{ref}})$$  \n",
        "\n",
        "But in PPO, we don’t directly maximize raw rewards. Instead, we maximize the **advantage** (how much better an action is compared to the expected baseline), computed using the **value model** \\( V(x) \\):  \n",
        "$$\n",
        "A(x, y) = r_\\phi(y|x) - V(x)\n",
        "$$\n",
        "\n",
        "The **true PPO objective** includes this advantage term:  \n",
        "$$\n",
        "L(\\theta) = \\mathbb{E} \\left[ \\min\\left( \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)} A(x, y), \\text{clip}\\left(\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)}, 1-\\epsilon, 1+\\epsilon\\right) A(x, y) \\right) \\right] - \\beta \\, \\text{KL}(\\pi_\\theta || \\pi_{\\text{ref}})\n",
        "$$\n",
        "\n",
        "Key differences:\n",
        "- **Advantage \\( A(x, y) \\)**: Requires the value model to estimate \\( V(x) \\).  \n",
        "- **Clipping**: Prevents overly large policy updates (the \"proximal\" in PPO).  \n",
        "\n",
        "\n",
        "### **2. Definition of `ratios`**\n",
        "The ratio is calculated as:\n",
        "$$\n",
        "\\text{ratios} = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\text{old}}(a_t \\mid s_t)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\pi_\\theta(a_t \\mid s_t)$: Probability of action $a_t$ (e.g., a generated token/response) under the **current policy** (with parameters $\\theta$).\n",
        "- $\\pi_{\\text{old}}(a_t \\mid s_t)$: Probability of the same action under the **old policy** (before the update).\n",
        "\n",
        "#### **Role in PPO Loss**\n",
        "The ratio is used to:\n",
        "1. **Weight the advantages** (how much better/worse an action is compared to the baseline):\n",
        "   $$\n",
        "   \\text{policy_loss} = -\\text{min}\\left(\n",
        "   \\text{ratios} \\times \\text{advantages}, \\quad\n",
        "   \\text{clipped_ratios} \\times \\text{advantages}\n",
        "   \\right)\n",
        "   $$\n",
        "   - If the ratio is high (>1), the action became more likely under the new policy.\n",
        "   - If the ratio is low (<1), the action became less likely.\n",
        "\n",
        "2. **Enforce trust-region updates** via clipping (`clip_eps`):\n",
        "   - Clipping the ratios (e.g., to $[1-0.2, 1+0.2]$) prevents overly large policy updates, ensuring stability.\n",
        "\n",
        "#### **Why Use Ratios?**\n",
        "- **Importance Sampling**: Allows reusing old trajectories (collected under $\\pi_{\\text{old}}$) to update the current policy ($\\pi_\\theta$) without resampling.\n",
        "- **Controlled Updates**: The clipping ensures the policy doesn’t change too drastically, avoiding catastrophic failures (e.g., generating gibberish to exploit the reward model).\n",
        "\n",
        "\n",
        "### **Key Points**\n",
        "1. **Advantages**: Tell us if an action was better ($A > 0$) or worse ($A < 0$) than expected.\n",
        "2. **Ratios**: Adjust policy updates based on how much the action’s probability changed.\n",
        "3. **Clipping**: Prevents aggressive updates (e.g., a ratio of 100x would be clipped to $1.2$ if `clip_eps=0.2`).\n",
        "\n",
        "#### **3. Where Does the Value Model Fit In?**\n",
        "1. **Value Function \\( V(x) \\)**:  \n",
        "   - The value model predicts the **expected future reward** from state \\( x \\) (e.g., a partially generated text).  \n",
        "   - Input: Partial sequence (`\"Q: 2+2? → A:\"`).  \n",
        "   - Output: Scalar (e.g., `0.3`).  \n",
        "\n",
        "2. **Advantage Calculation**:  \n",
        "   - If the reward for `\"A: 5\"` is `-1.0` (toxic) and \\( V(x) = 0.3 \\), then:  \n",
        "     $$\n",
        "     A(x, y) = -1.0 - 0.3 = -1.3\n",
        "     $$  \n",
        "   - A negative advantage pushes the policy away from toxic outputs.  \n",
        "\n",
        "3. **Training the Value Model**:  \n",
        "   - The value model is trained separately to minimize:  \n",
        "     $$\n",
        "     L(V) = \\mathbb{E} \\left[ (V(x) - R)^2 \\right]\n",
        "     $$  \n",
        "     where \\( R \\) is the **actual discounted return** (e.g., the final reward).  \n",
        "\n",
        "\n",
        "#### **4. Why the difference?**\n",
        "\n",
        "- In RLHF, rewards are often sparse (e.g., per-output human feedback), so the value model’s role is sometimes simplified.  \n",
        "- In classic PPO (e.g., for robotics), the value model is critical for dense reward signals.  \n",
        "\n",
        "#### **5. Practical Implications for our Setup**\n",
        "With `gpt2` (policy) and `toxic-bert` (reward model):\n",
        "1. **Value Model**:  \n",
        "   - Add a regression head to `gpt2` (shared backbone with policy).  \n",
        "   - Train it to predict rewards for partial sequences.  \n",
        "\n",
        "2. **Modified Objective**:  \n",
        "   ```python\n",
        "    # Pseudocode for PPO step (simplified)\n",
        "    advantages = rewards - values  # A(s,a) = R(s,a) - V(s)\n",
        "    ratios = current_probs / old_probs  # π_θ(a|s) / π_old(a|s)\n",
        "\n",
        "    policy_loss = -torch.min(\n",
        "        ratios * advantages,  # Unclipped objective\n",
        "        torch.clamp(ratios, 1-clip_eps, 1+clip_eps) * advantages  # Clipped objective\n",
        "    )\n",
        "\n",
        "    kl_penalty = beta * kl_div(policy, ref_model)  # Penalty for diverging from reference\n",
        "    total_loss = policy_loss + kl_penalty\n",
        "   ```\n",
        "\n",
        "\n",
        "#### **Key Takeaways**\n",
        "| Component       | Role in Objective                                                                 |\n",
        "|----------------|-----------------------------------------------------------------------------------|\n",
        "| **Reward Model** ($ r_\\phi $) | Provides $ r(y\\|x) $ (e.g., toxicity scores).                                  |\n",
        "| **Value Model** ($ V $)       | Estimates \\( V(x) \\) to compute advantages $ A(x, y) = r(y\\|x) - V(x) $.       |\n",
        "| **Reference Model** ($ \\pi_{\\text{ref}} $) | Anchors KL divergence to prevent over-optimization.                             |\n",
        "| **Policy** ($ \\pi_\\theta $)   | Updated to maximize advantages while staying close to $ \\pi_{\\text{ref}} $.    |\n",
        "\n",
        "**The simplified objective works for RLHF**, but for true PPO, include advantages (and clipping). The value model is essential for this!  \n"
      ],
      "metadata": {
        "id": "E2znShy6VYiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A2. Reward Function at Inference Time**\n",
        "\n",
        "After training a **reinforcement learning (RL) model**, the **reward function is no longer needed during deployment** if the policy has been fully optimized and operates in a static environment. However, there are important exceptions and nuances:\n",
        "\n",
        "**When the Reward Function is Still Needed After Training**\n",
        "1. **Online Learning / Continual Adaptation**  \n",
        "   - If the RL agent keeps learning during deployment (e.g., adapting to new environments), the reward function must remain active to provide feedback for updates.  \n",
        "   - Example: A recommendation system that continuously refines its policy based on user clicks (rewards).\n",
        "\n",
        "**Safe Exploration & Monitoring**  \n",
        "   - In safety-critical applications (e.g., autonomous driving), the reward function may be used to monitor performance and trigger failsafes if rewards drop unexpectedly.  \n",
        "\n",
        "**Reward as a Diagnostic Tool**  \n",
        "   - Even if the policy is fixed, the reward function can evaluate performance post-deployment (e.g., detecting distributional shift or performance degradation).  \n",
        "\n",
        "**When the Reward Function is *Not* Needed**\n",
        "- **Static Environments with Fixed Policies**  \n",
        "  - Once trained, the policy (e.g., a neural network) can act independently, mapping states → actions without reward calculations.  \n",
        "  - Example: A game-playing AI (like AlphaGo) uses its pre-trained policy without recomputing rewards during matches.  \n",
        "\n",
        "**Key Distinction: Reward Function vs. Policy**\n",
        "- **Reward Function**: Guides *training* (like a teacher’s feedback).  \n",
        "- **Policy**: The final *executable strategy* (like a student’s learned skills).  \n",
        "\n",
        "**Practical Implications**\n",
        "- If your environment is dynamic or requires ongoing learning, keep the reward function.  \n",
        "- For static tasks, the trained policy alone suffices."
      ],
      "metadata": {
        "id": "DnYeGI3lIE3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A3. RL approaches for LLMs**\n",
        "\n",
        "\n",
        "Several **reinforcement learning (RL) approaches** are used to fine-tune large language models (LLMs) beyond **PPO** and **GRPO**, each with distinct advantages for alignment, efficiency, or stability. Here’s a breakdown of key alternatives:\n",
        "\n",
        "\n",
        "**1. Direct Preference Optimization (DPO)**\n",
        "- **What it does**: Replaces traditional RL with a **closed-form policy update** using human preference data (no reward model or PPO required).  \n",
        "- **Advantages**:  \n",
        "  - Simpler and more stable than PPO (avoids reward model biases).  \n",
        "  - Directly optimizes for human preferences via pairwise rankings.  \n",
        "- **Used in**: Zephyr, Mistral-7B, and other preference-tuned LLMs.  \n",
        "- **Paper**: [Rafailov et al. (2023)](https://arxiv.org/abs/2305.18290).\n",
        "\n",
        "\n",
        "**2. Reinforcement Learning from Human Feedback (RLHF) Variants**\n",
        "#### **a. Advantage-Weighted Regression (AWR)**\n",
        "  - **What it does**: Uses **advantage-weighted loss** (like PPO but simpler).  \n",
        "  - **Advantages**: More stable for offline RL (no on-policy sampling needed).  \n",
        "  - **Used in**: Early LLM alignment (e.g., OpenAI’s pre-ChatGPT models).  \n",
        "\n",
        "#### **b. Q-Learning (e.g., DQN, CQL)**\n",
        "  - **What it does**: Learns a **Q-function** to score actions (useful for constrained generation).  \n",
        "  - **Advantages**: Better for **discrete action spaces** (e.g., choosing among template responses).  \n",
        "  - **Limitations**: Rarely used for full LLM fine-tuning (scalability issues).  \n",
        "\n",
        "\n",
        "**3. Contrastive Learning Methods**\n",
        "#### **a. Sequence Likelihood Calibration (SLiC)**\n",
        "  - **What it does**: Uses **contrastive ranking loss** (like DPO but simpler).  \n",
        "  - **Advantages**: No RL loop needed; works with static datasets.  \n",
        "  - **Used in**: Lightweight alignment (e.g., [Yuan et al. (2023)](https://arxiv.org/abs/2305.20050)).  \n",
        "\n",
        "#### **b. RankRLHF**\n",
        "  - **What it does**: Extends DPO to **listwise rankings** (A > B > C > D).  \n",
        "  - **Advantages**: Better for multi-response ranking scenarios.  \n",
        "\n",
        "\n",
        "**4. Offline RL Methods**\n",
        "#### **a. Implicit Language Q-Learning (ILQL)**\n",
        "  - **What it does**: Combines Q-learning with LLMs for **offline preference data**.  \n",
        "  - **Advantages**: Efficient for **constrained text generation** (e.g., avoiding harmful outputs).  \n",
        "  - **Used in**: [Snorkel AI’s work](https://arxiv.org/abs/2206.11871).  \n",
        "\n",
        "#### **b. Conservative Q-Learning (CQL)**\n",
        "  - **What it does**: Penalizes overestimation of Q-values in offline data.  \n",
        "  - **Advantages**: Reduces hallucination in RL-tuned LLMs.  \n",
        "\n",
        "\n",
        "**5. Hybrid Approaches**\n",
        "#### **a. Reinforced Self-Training (ReST)**\n",
        "  - **What it does**: Iteratively generates samples, filters best ones, and fine-tunes on them.  \n",
        "  - **Advantages**: No reward model needed (self-improving loop).  \n",
        "  - **Used in**: Google’s [GEMINI](https://arxiv.org/abs/2308.08998).  \n",
        "\n",
        "#### **b. Expert Iteration (ExIt)**\n",
        "  - **What it does**: Alternates between LLM generations and expert feedback (like AlphaGo).  \n",
        "  - **Advantages**: Useful for **code-generating LLMs** (e.g., GitHub Copilot).  \n",
        "\n",
        "\n",
        "#### **When to Use Which?**\n",
        "| **Method**       | **Best For**                           | **Complexity** |  \n",
        "|------------------|----------------------------------------|---------------|  \n",
        "| **PPO**         | High-resource RLHF (e.g., ChatGPT)     | High          |  \n",
        "| **DPO**         | Lightweight preference tuning          | Low           |  \n",
        "| **SLiC/ILQL**   | Offline data + no reward model         | Medium        |  \n",
        "| **Q-Learning**  | Discrete action spaces (e.g., dialog)  | High          |  \n",
        "\n",
        "\n",
        "\n",
        "#### **Emerging Trends**\n",
        "- **Multimodal RLHF**: Extending RL to align LLMs with vision/audio (e.g., GPT-4V).  \n",
        "- **Adversarial RL**: Using RL to **red-team** LLMs (e.g., training against jailbreaks).  \n",
        "\n",
        "#### **Group Relative Policy Optimization (GRPO)**\n",
        "\n",
        "**Group Relative Policy Optimization (GRPO)** is a recent **reinforcement learning (RL) approach**, specifically designed for **policy optimization** in scenarios where relative performance comparisons matter (e.g., aligning AI behavior with human preferences). DeepSeek uses this approach.\n",
        "\n",
        "**Key Features of GRPO**  \n",
        "1. **Relative Policy Optimization**  \n",
        "   - Unlike standard RL (which maximizes absolute rewards), GRPO focuses on **relative comparisons** (e.g., \"Is response A better than B?\"), making it suitable for **human feedback-driven RL (RLHF)**.\n",
        "   - Similar in spirit to **Pairwise Preference Optimization** methods (e.g., DPO, PPO with ranking-based rewards).  \n",
        "\n",
        "2. **Group-Wise Learning**  \n",
        "   - Operates on **batches of trajectories** (e.g., multiple LLM responses ranked by humans/reward models).  \n",
        "   - Optimizes policies by comparing **groups of actions** rather than individual rewards.  \n",
        "\n",
        "3. **Connection to Existing Methods**  \n",
        "   - Can be seen as a **generalization of PPO** (Proximal Policy Optimization) but with **ranking-based objectives** instead of absolute rewards.  \n",
        "   - Shares similarities with **Off-Policy RL** (e.g., learns from logged human preference data).  \n",
        "\n",
        "**How GRPO Differs from Traditional RL**\n",
        "\n",
        "| **Aspect**          | **Standard RL (e.g., PPO)**            | **GRPO**                          |  \n",
        "|----------------------|---------------------------------------|-----------------------------------|  \n",
        "| **Reward Source**    | Absolute scalar rewards               | Relative rankings (A > B > C)     |  \n",
        "| **Objective**        | Maximize expected reward              | Maximize likelihood of preferred trajectories |  \n",
        "| **Data Usage**       | Requires dense rewards                | Works with sparse pairwise preferences |  \n",
        "| **Use Case**         | Game-playing, robotics                | RLHF, alignment tasks (e.g., LLMs) |  \n",
        "\n",
        "**Is GRPO Used in Practice?**  \n",
        "- While **PPO** remains the dominant choice for RLHF (e.g., ChatGPT fine-tuning), GRPO is a **research-level method** (e.g., proposed in papers like [\"Relative Policy Optimization\"](https://arxiv.org/abs/2305.18239)) aimed at improving stability in preference-based RL.  \n",
        "- It’s part of a broader trend toward **ranking-based RL** (e.g., DPO, SLiC, and contrastive learning variants).  \n",
        "\n",
        "**When to Consider GRPO?**  \n",
        "- If you’re working on **RLHF with human/AI-generated rankings** (vs. explicit rewards).  \n",
        "- If standard PPO struggles with **sparse or noisy preference data**.  \n",
        "\n",
        "Would you like a high-level pseudocode example of how GRPO updates policies?"
      ],
      "metadata": {
        "id": "oTRfKJltOJQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Complete Training Script (Collapsed)\n",
        "Markdown(\"\"\"\n",
        "Full implementation would include:\n",
        "1. Proper dataset tokenization and batching\n",
        "2. Complete reward model training loop\n",
        "3. Full PPO implementation with value head\n",
        "4. Comprehensive evaluation metrics\n",
        "5. Hyperparameter tuning\n",
        "6. Safety-specific loss functions\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "rIdzHngl0OeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "iMggEnAq6GPA"
      }
    }
  ]
}